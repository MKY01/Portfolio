---
title: "BDA 4"
author: "Man Kit Yip"
date: "26/12/2017"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

```{r}
#1. SVM (2%) [Textbook 9.7.8]
##This problem involves the OJ data set which is part of the ISLR package.

library(ISLR)
OJ
```

```{r}
##(a) Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.

set.seed(0)
train <- sample(dim(OJ)[1],800)
OJ.train <- OJ[train, ]
OJ.test <- OJ[-train, ]
```

```{r}
##(b) Fit a support vector classifier to the training data using cost=0.01, with Purchase as the response and the other variables as predictors. Use the summary() function to produce summary statistics, and describe the results obtained.

set.seed(0)
svm.fit <- svm(Purchase ~., data = OJ.train, kernel = "linear", cost = 0.01)
summary(svm.fit)
```

```{r}
##(c) What are the training and test error rates?

set.seed(0)
train.y_hat <- predict(svm.fit, OJ.train)
table(OJ.train$Purchase, train.y_hat)

(54+79)/(437+54+79+230)

###The training error rates is around 0.166 or 16.6%.
```
```{r}
set.seed(0)
test.y_hat <- predict(svm.fit, OJ.test)
table(OJ.test$Purchase, test.y_hat)

(15+23)/(147+15+23+85)

###The test error rates is around 0.141 or 14.1%.
```

```{r}
##(d) Use the tune() function to select an optimal cost. Consider values in the range 0.01 to 10.

set.seed(1)
tune.out2 <- tune(svm, Purchase ~., data = OJ.train, kernel = "linear", ranges = list(cost = c(0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10)))
summary(tune.out2)

###I can conclude that the optimal cost is at 0.01.
```

```{r}
##(e) Compute the training and test error rates using this new value for cost.

set.seed(1)
svm.train2 <- svm(Purchase ~ ., data = OJ.train, kernel = "linear", cost = tune.out2$best.parameters$cost)
train.y_hat2 <- predict(svm.fit, OJ.train)
table(OJ.train$Purchase, train.y_hat2)

(54+79)/(437+54+79+230)

###The training error rates is also around 0.166 or 16.6%. (As the costs were the same as before tune.)
```

```{r}
set.seed(1)
test.y_hat2 <- predict(svm.fit, OJ.test)
table(OJ.test$Purchase, test.y_hat2)

(15+23)/(147+15+23+85)

###The test error rates is also around 0.141 or 14.1%. (As the costs were the same as before tune.)
```

```{r}
##(f) Repeat parts (b) through (e) using a support vector machine with a radial kernel. Use the default value for gamma.

set.seed(2)
svm.fit3 <- svm(Purchase ~ ., data = OJ.train, kernel = "radial", gamma = 1)
summary(svm.fit3)
```

```{r}
set.seed(2)
train.y_hat3 <- predict(svm.fit3, OJ.train)
table(OJ.train$Purchase, train.y_hat3)

(54+32)/(459+32+54+255)

###The training error rates is 0.108 or 10.8%.
```

```{r}
set.seed(2)
test.y_hat3 <- predict(svm.fit3, OJ.test)
table(OJ.test$Purchase, test.y_hat3)

(20+33)/(142+20+33+75)

###The test error rates is around 0.196 or 19.6%
```

```{r}
set.seed(3)
tune.out4 <- tune(svm, Purchase ~., data = OJ.train, kernel = "radial", ranges = list(cost = c(0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10)))
summary(tune.out4)

###I can conclude that the optimal cost is at 1.
```

```{r}
set.seed(3)
svm.train3 <- svm(Purchase ~ ., data = OJ.train, kernel = "radial", cost = tune.out4$best.parameters$cost)
train.y_hat3 <- predict(svm.fit3, OJ.train)
table(OJ.train$Purchase, train.y_hat3)

(54+32)/(459+54+32+255)

###The training error rates is around 0.108 or 10.8%. (as the costs were the same as before)
```

```{r}
set.seed(3)
test.y_hat3 <- predict(svm.fit3, OJ.test)
table(OJ.test$Purchase, test.y_hat3)

(33+20)/(142+20+33+75)

###The test error rates is also around 0.196 or 19.6%.(as the costs were the same as before)
```


```{r}
##(g) Repeat parts (b) through (e) using a support vector machine with a polynomial kernel. Set degree=2.

set.seed(4)
svm.fit5 <- svm(Purchase ~ ., data = OJ.train, kernel = "polynomial", degree = 2)
summary(svm.fit5)
```
```{r}
set.seed(4)
train.y_hat4 <- predict(svm.fit5, OJ.train)
table(OJ.train$Purchase, train.y_hat4)

(103+37)/(453+103+37+206)

###The train data error rate is around 0.175 or 17.5%.
```
```{r}
set.seed(4)
test.y_hat4 <- predict(svm.fit5, OJ.test)
table(OJ.test$Purchase, test.y_hat4)

(42+14)/(148+44+14+66)

###The test data error rate is around 0.206 or 20.6%.
```
```{r}
set.seed(5)
tune.out6 <- tune(svm, Purchase ~., data = OJ.train, kernel = "polynomial", ranges = list(cost = c(0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10)))
summary(tune.out6)

###I can conclude that the optimal cost is at 2.
```
```{r}
set.seed(5)
svm.train4 <- svm(Purchase ~ ., data = OJ.train, kernel = "polynomial", cost = tune.out6$best.parameters$cost)
train.y_hat5 <- predict(svm.fit5, OJ.train)
table(OJ.train$Purchase, train.y_hat5)

(77+58)/(433+58+77+232)

###The training error rates is around 0.169 6or 16.9%.
```
```{r}
set.seed(5)
test.y_hat5 <- predict(svm.fit5, OJ.test)
table(OJ.test$Purchase, test.y_hat5)

(25+18)/(144+18+25+83)

###The test error rates is also around 0.159 or 15.9%.
```

```{r}
##(h) Overall, which approach seems to give the best results on this data?

###'Linear' approach, using the parameter costs = 0.01:
###train error rate = 16.6% and test error rate = 14.1%.

###'Linear' approach, using the 'optimal' parameter:
###train error rate = 16.6% and test error rate = 14.1%.

###'Radial' approach, gamma = 1:
###train error rate = 10.8% and test error rate = 19.6%.

###'Radial' approach, using the 'optimal' parameter:
###train error rate = 10.8% and test error rate = 19.6%.

###'Polynomial' approach, degree = 2:
###train error rate = 17.5% and test error rate = 20.6%.

###'Polynomial' approach, using the 'optimal' parameter:
###train error rate = 16.9% and test error rate = 15.9%.

###Overall, the 'Radial' approach seems to give the best results on this data, suggesting that it fits the pattern the most.
```

```{r}
#2. [OPTIONAL] SVM and Logistic Regression (0%) [Textbook 9.7.5]
##This question is optional.
##We have seen that we can fit an SVM with a non-linear kernel in order to perform classification using a non-linear decision boundary. We will now see that we can also obtain a non-linear decision boundary by performing logistic regression using non-linear transformations of the features.

```

```{r}
##(a) Generate a data set with n = 500 and p = 2, such that the observations belong to two classes with a quadratic decision boundary between them. For instance, you can do this as follows:
##𝑥1=𝑟𝑢𝑛𝑖𝑓(500)−0.5
##𝑥2=𝑟𝑢𝑛𝑖𝑓(500)−0.5
##𝑦=1∗(𝑥12−𝑥22>0)

```

```{r}
##(b) Plot the observations, colored according to their class labels. Your plot should display 𝑋1 on the x-axis, and 𝑋2 on the y-axis.
```

```{r}
##(c) Fit a logistic regression model to the data, using 𝑋1 and 𝑋2 as predictors.

```

```{r}
##(d) Apply this model to the training data in order to obtain a predicted class label for each training observation. Plot the observations, colored according to the predicted class labels. The decision boundary should be linear.

```

```{r}
##(e) Now fit a logistic regression model to the data using non-linear functions of 𝑋1 and 𝑋2 as predictors (e.g. 𝑋12, 𝑋1×𝑋2, log(𝑋2), and so forth)

```

```{r}
##(f) Apply this model to the training data in order to obtain a predicted class label for each training observation. Plot the observations, coloured according to the predicted class labels. The decision boundary should be obviously non-linear. If it is not, then repeat (a)-(e) until you come up with an example in which the predicted class labels are obviously non-linear.

```

```{r}
##(g) Fit a support vector classifier to the data with 𝑋1 and 𝑋2 as predictors. Obtain a class prediction for each training observation. Plot the observations, colored according to the predicted class labels.

```

```{r}
##(h) Fit a SVM using a non-linear kernel to the data. Obtain a class prediction for each training observation. Plot the observations, coloured according to the predicted class labels.

```

```{r}
##(i) Comment on your results.

```

```{r}
#3. Hierarchical Clustering (1%) [Textbook 10.7.9]
##Consider the USArrests data. We will now perform hierarchical clustering on the states.
##(a) Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states.

library(ISLR)
set.seed(0)
USArrests
hc.complete <-hclust(dist(USArrests), method="complete")
plot(hc.complete)
```

```{r}
##(b) Cut the dendrogram at a height that results in three distinct clusters. Which states belong to which clusters?

cutree(hc.complete,  3)

###The following states belongs to cluster 1: Alabama, Alaska, Arizona, California, Delaware, Florida, Illinois, Louisiana, Maryland, Michigan, Mississippi, Nevada, New Mexico, New York, North Carolina, South Carolina.

###The following states belongs to cluster 2: Arkansas, Colorado, Georgia, Massachusetts, Missouri, New Jersey, Oklahoma, Oregon, Rhode Island, Tennessee, Texas, Virginia, Washington, Wyoming.

###The following states belongs to cluster 3: Connecticut, Hawaii, Idaho, Indiana, Iowa, Kansas, Kentucky, Maine, Minnesota, Montana, Nebraska, New Hampshire, North Dakota, Ohio, Pennsylvania, South Dakota, Utah, Vermont, West Virginia, Wisconsin.
```

```{r}
##(c) Hierarchically cluster the states using complete linkage and Euclidean distance, after scaling the variables to have standard deviation one.

usa.sc <-scale(USArrests)
hc.complete.sc <- hclust(dist(usa.sc), method="complete")
plot(hc.complete.sc)
```

```{r}
##(d) What effect does scaling the variables have on the hierarchical clustering obtained? In your opinion, should the variables be scaled before the inter-observation dissimilarities are computed? Provide a justification for your answer.

cutree(hc.complete.sc, 3)
table(cutree(hc.complete, 3),cutree(hc.complete.sc, 3))
```
```{r}
###As you can see, scaling it affects the clusters obtained as shown by the assymetry of the table which are somewhat different. The variables should be scaled before the inter-observation dissimilarities are computed, as they have different units of measurement.
```

```{r}
#4. PCA and K-Means Clustering (2%) [Textbook 10.7.10]
##In this problem, you will generate simulated data, and then perform PCA and K-means clustering on the data.

##(a) Generate a simulated data set with 20 observations in each of three classes (i.e. 60 observations total), and 50 variables.

set.seed(1)
x = matrix(rnorm(20*3*50, mean = 0, sd = 0.001), ncol = 50)
x[1:20, 2] = 1
x[21:40, 1] = 2
x[21:40, 2] = 2
x[41:60, 1] = 1
class.labels<- c(rep(1,20), rep(2,20), rep(3,20))

##Hint: There are a number of functions in R that you can use to generate data. One example is the rnorm() function; runif() is another option. Be sure to add a mean shift to the observations in each class so that there are three distinct classes.
```

```{r}
##(b) Perform PCA on the 60 observations and plot the first two principal components' eigenvector. Use a different color to indicate the observations in each of the three classes. If the three classes appear separated in this plot, then continue on to part (c). If not, then return to part (a) and modify the simulation so that there is greater separation between the three classes. Do not continue to part (c) until the three classes show at least some separation in the first two principal component eigenvectors.

pr.out <- prcomp(x)
plot(pr.out$x[ ,1:2], col= 1:3, pch = 19, xlab = "Z1", ylab = "Z2")
```

```{r}
##(c) Perform K-means clustering of the observations with K = 3. How well do the clusters that you obtained in K-means clustering compare to the true class labels?

km.out <- kmeans(x, 3, nstart = 20)
table(class.labels, km.out$cluster)

###The observations appears to be in 3 evenly distributed clusters.

##Hint: You can use the table() function in R to compare the true class labels to the class labels obtained by clustering. Be careful how you interpret the results: K-means clustering will arbitrarily number the clusters, so you cannot simply check whether the true class labels and clustering labels are the same.
```

```{r}
##(d) Perform K-means clustering with K = 2. Describe your results.

km.out <- kmeans(x, 2, nstart = 20)
table(class.labels, km.out$cluster)

###The observations appears to have been merged into 1 of the 2 clusters.
```

```{r}
##(e) Now perform K-means clustering with K = 4, and describe your results.

km.out <- kmeans(x, 4, nstart = 20)
table(class.labels, km.out$cluster)

###The first cluster has split into 2 smaller clusters.
```

```{r}
##(f) Now perform K-means clustering with K = 3 on the first two principal components, rather than on the raw data. That is, perform K-means clustering on the 60 × 2 matrix of which the first column is the first principal component's corresponding eigenvector, and the second column is the second principal component's corresponding eigenvector. Comment on the results.

km.out <- kmeans(pr.out$x[ , 1:2], 3, nstart = 20)
table(class.labels, km.out$cluster)

###The observations appears to be evenly distributed as well, perhaps this is because the ratios were kept the same.
```

```{r}
##(g) Using the scale() function, perform K-means clustering with K = 3 on the data after scaling each variable to have standard deviation one . How do these results compare to those obtained in (b)? Explain.

km.out <- kmeans(scale(x), 3, nstart = 20)
table(class.labels, km.out$cluster)

###The scaled data appears to be poorly clustered. Perhaps scaling affected the distance between the observations.
```

```{r}
#Bibliography and references:

##Gareth, J., 2017. An introduction to Statistical learning. 8th ed. New York: Springer

##Han, T. 2017. Big Data Analytics using R. [Lectures to MSc Data Science]. Birkbeck college, University of London, Dec 17.

##N/A., 2017. ISLR. URL https://rpubs.com/. (accessed 30.12.17)

##James, G., 2014. Chapter 9 Lab: Principal component analysis [WWW Document]. An Introduction to Statistical Learning with Applications in R. URL http://www-bcf.usc.edu/~gareth/ISL/Chapter%209%20Lab.txt (accessed 2.1.18).

##James, G., 2014. Chapter 10 Lab: Principal component analysis [WWW Document]. An Introduction to Statistical Learning with Applications in R. URL http://www-bcf.usc.edu/~gareth/ISL/Chapter%2010%20Labs.txt (accessed 3.1.18).

#End of document
```

